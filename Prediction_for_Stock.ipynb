{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>股票之預測</center>\n",
    "\n",
    "- @author: Vincent Yau\n",
    "- @version: 1.0 | 2018/05/21\n",
    "- @email: wenhengqiu@gmail.com\n",
    "\n",
    "### 数据前处理\n",
    "- 从一堆数据中获得与台股2912相关的文章，分为2016年及2017年\n",
    "- 需要解决数据中NaN的问题，尤其是用正则匹配中文字后，数据为空的情况\n",
    "- 我选择的处理方法：\n",
    "    - 1、对DataFrame中的NaN，采用dropna()方法\n",
    "    - 2、对于分词中的空值，赋予一个生字string（不是一个好的方法，但因为我们选好了特征词，所以一个字符对结果影响不大[说明还是有影响]）\n",
    "    - 2、对于terms的tfidf的缺失，采用赋予0.0001（不是一个好方法）\n",
    "\n",
    "### 程式逻辑整理\n",
    "- 讀取某一年的所有文章，獲得所有的日期，存為 dateList\n",
    "- 獲得某一天的所有文章，每篇文章都作為list的元素保存起來 articlesList，然後直接存入{日期:articlesLsit}字典中\n",
    "    - 這裡要注意的是，盡量對數據只進行一次讀取，一次讀取多次使用，避免因為頻繁讀取造成了資源浪費\n",
    "- 對articlesList中的每一篇文章進行分詞處理，單獨計算其 Term Frequency，保存成 Dictionary\n",
    "    - 使用jieba分詞，而不用N-gram是為了提高分詞的準確性，且可針對經濟新聞定制Stop Words列表\n",
    "    - 採用jieba之前，對傳入的characters進行正則匹配，獲取其中所有的中文\n",
    "    - 中文正則表達式為[^\\u4e00-\\u9fa5]，故當string無中文時會產生NaN，解決方法參考數據前處理\n",
    "- 計算某一天所有文章詞的tf-idf，整合成一個dict保存起來\n",
    "- 獲得特征詞表（1000個詞）\n",
    "- 將每天的文章按照 [特征詞 + tfidf]進行表示，將所有的天數的數據都append到一個List中\n",
    "\n",
    "### 特征詞的獲得\n",
    "- 將所有的文章，根據股票的漲跌判斷，分為「看漲文章」「看跌文章」\n",
    "- 分別將兩類文章進行Features Selection（特征詞抽取），這裡抽取的依據是TF-IDF，目測抽取1000個詞\n",
    "- 注意特征詞必須要有順序\n",
    "\n",
    "### 股票漲跌判斷\n",
    "- 若第x天的股價，與x+n天的股價存在一定比例的漲or跌，則將x天的文章列為漲or跌文章（估計我們會分為漲、跌、不漲不跌）\n",
    "- Actually，這裡直接將漲跌的label與日期對應起來即可\n",
    "- 漲跌的判斷在Machine learning中，屬於對數據進行標標籤，因此，我們的模型屬於：監督式學習\n",
    "- 理論上，若將每篇文章都進行此標記，則會增Traning Dataset，但因數據文章有許多comments，如此操作會造成大量的稀疏數據\n",
    "- 將某天的所有文章，用list合併一起，并最終用「特征詞：tfidf」進行表示，是現今採用的方式\n",
    "\n",
    "### 分類器的構建\n",
    "- MultinomialNB\n",
    "- Sppport Vector Machine\n",
    "- GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba.analyse\n",
    "from sklearn import svm\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、分詞與計算tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jieba中文分詞，并獲取tfidf排名前3000詞（未採用），還要解決詞與vocabulary匹配的問題\n",
    "def jieba_Chinese(articles):\n",
    "    all_articles2 = []\n",
    "    #使用正则选取文章中汉字的部分, 使用str()是为了防止TypeError: expected string or bytes-like object\n",
    "    new_article = re.sub(r'[^\\u4e00-\\u9fa5]', '', str(articles))\n",
    "    if new_article == '':\n",
    "        new_article = '㐓'\n",
    "#     new_article2 = jieba.analyse.extract_tags(new_article, topK=3000, withWeight=True, allowPOS=())\n",
    "    new_article2 = jieba.cut(new_article, cut_all=False)\n",
    "    for word in new_article2:\n",
    "        all_articles2.append(word)\n",
    "    articles_dataFrame = DataFrame(all_articles2)[0].value_counts().to_dict()\n",
    "    return articles_dataFrame\n",
    "\n",
    "#傳入某天的aticlesList，獲得分詞后的terms和其term frequency\n",
    "def get_termList_forArticles(aticlesList):\n",
    "    termList = []\n",
    "    for sentens in aticlesList:\n",
    "        oneArticleTerm = jieba_Chinese(sentens)\n",
    "        termList.append(oneArticleTerm)\n",
    "    return termList\n",
    "\n",
    "#Calculate tf(means Collection Frequency)、Document Frequency\n",
    "def getFrequency(termList):\n",
    "    #Store all terms' Document Frequency into allTermDocFre\n",
    "    allTermDocFre = {}\n",
    "    \n",
    "    #Store Collection Frequency for terms into termFrequency\n",
    "    termFrequency = {}\n",
    "\n",
    "    for content in termList:\n",
    "        for term in content:\n",
    "            if term in allTermDocFre:\n",
    "                termFrequency[term] = termFrequency[term] + content[term]\n",
    "                allTermDocFre[term] += 1\n",
    "            else:\n",
    "                termFrequency[term] = content[term]\n",
    "                allTermDocFre[term] = 1\n",
    "    return allTermDocFre, termFrequency\n",
    "\n",
    "#Convert list to Series\n",
    "def getSeries(list, names, indexName):\n",
    "    h = pd.Series(list, name=names)\n",
    "    h.index.name = indexName\n",
    "    return h.reset_index()\n",
    "\n",
    "#tf-idf\n",
    "def calTfidf(allTermDocFre, termFrequency, records):\n",
    "    # The bank's counts is 1830 less than the number of all articles beacause we delete some terms(tf<3 in single article)\n",
    "    h1 = getSeries(allTermDocFre, 'df', 'Term')\n",
    "    h2 = getSeries(termFrequency, 'tf', 'Term')\n",
    "    #这里可以控制输出terms的长度\n",
    "    data = pd.merge(h1, h2).sort_values(by='tf', ascending=False)\n",
    "    \n",
    "    numOfRecords = len(records)\n",
    "    numOfTerms = len(allTermDocFre)\n",
    "    #math.log only accepts single float value, So you should use np.log here\n",
    "    data['tf-idf'] =  np.log(numOfRecords / data['df']) * (data['tf'] / numOfTerms)\n",
    "    \n",
    "    #删除某一列的方法为：data.pop('tf')，原数据中不包含'tf'\n",
    "\n",
    "    #重置索引为Term, 并将以'tf-idf列为基础转为字典\n",
    "    data = data.set_index(['Term']).to_dict()['tf-idf']    \n",
    "    return data\n",
    "\n",
    "#A function of tfidf for the terms from articles\n",
    "def get_tfidf_for_articlesList(termList):\n",
    "    testTermsResult = get_termList_forArticles(termList)\n",
    "    allTermDocFre, termFrequency = getFrequency(testTermsResult)\n",
    "    terms_tfidf = calTfidf(allTermDocFre, termFrequency, termList)\n",
    "    return terms_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、獲得特征詞及構建向量空間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#输入特征词 及 terms及其tfidf构成的字典，输出按照特征词词序排列的tfidf列表\n",
    "def tfidfList_oneArticle(vocabulary, terms_tfidf):\n",
    "    list_for_oneArticle = []\n",
    "    for word in vocabulary:\n",
    "        if word in terms_tfidf.keys():\n",
    "            list_for_oneArticle.append(float(terms_tfidf[word]))\n",
    "        else:\n",
    "            #如果文章中没有某个词，就将其tfidf赋值为0.0001，随便设置的一个数值（远远小于正常的tf-idf）\n",
    "            #这里的赋值要记住，最后是用数值做计算，所以不可传入string\n",
    "            list_for_oneArticle.append(float(0.0001))\n",
    "    return list_for_oneArticle\n",
    "\n",
    "#將每天當做字典的key，每天的文章list當做value，存成dict，好處只需要讀一遍所有的文章\n",
    "def get_article_by_dict(articles, dates):\n",
    "    #先初始化每個日期，然後添加list\n",
    "    articles_dict = {}\n",
    "    for date in dates:\n",
    "        articles_dict[date] = []\n",
    "\n",
    "    for index, row in articles.iterrows():\n",
    "        if row['date'] in dates:\n",
    "#             print(row['date'])\n",
    "            articles_dict[row['date']].append(row['article'])\n",
    "    return articles_dict\n",
    "\n",
    "#这里每次找出某个date的文章时候，都会扫描一遍aticles_2017，造成效率降低、开销大增，需要解决这个问题\n",
    "def all_tfidf_list_forModel(dates, articlesDict, vocabualry):\n",
    "    all_tfidf_list = []\n",
    "    for date in dates_2017:\n",
    "        allArticlesforOneDay = articles_dict[date]\n",
    "        terms_tfidf = get_tfidf_for_articlesList(allArticlesforOneDay)\n",
    "        list_for_oneArticle = tfidfList_oneArticle(feature_words_selection, terms_tfidf)\n",
    "        all_tfidf_list.append(list_for_oneArticle)\n",
    "        print('Caculation of %s is ok' % date)\n",
    "    return all_tfidf_list\n",
    "\n",
    "#计算预测的正确率\n",
    "def trueRate(prediction, label):\n",
    "    length = len(prediction)\n",
    "    greatRate = 0\n",
    "    for i in range(length):\n",
    "        if prediction[i] == label[i]:\n",
    "            greatRate += 1\n",
    "    return greatRate/length\n",
    "# #传入的值包括每条数据的label，向量词库，每条数据的tfdif\n",
    "# def data_for_classfier(label, tfidfm, vocabulary):\n",
    "#     #tfidfm为tfidf的矩阵，即词向量空间实例，到此为止，比较重要的是tfidfm，与vocabulary\n",
    "#     #测试集与训练集的格式相似，即有相同的词向量空间Vocabulary，但tfidfm不同。\n",
    "#     bunch = Bunch(label=[], tfidfm=[], vocabulary={})\n",
    "#     bunch.label = label\n",
    "#     bunch.tfidfm = tfidfm\n",
    "#     bunch.vocabulary = vocabulary\n",
    "#     return bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_tfidf_list(path_aricles, path_vocabulary, path_label): \n",
    "    #获得所有文章，去除數據中的NaN值, axis=0代表去掉有空值的行\n",
    "    articles_2017 = pd.read_excel(path_aricles).dropna(axis=0)\n",
    "\n",
    "    #获取特征词list\n",
    "    feature_words_selection = list(pd.read_excel(path_vocabulary, squeeze=True))\n",
    "\n",
    "    all_label = pd.read_excel(path_label)\n",
    "    #獲得所有數據的label\n",
    "    articles_label = list(all_label['label'])\n",
    "    #獲得所有的日期\n",
    "    dates_2017 = list(all_label['dates'])\n",
    "\n",
    "    #獲得{日期:文章list}字典,最后一个月是dates_2017[202:]\n",
    "    articles_dict = get_article_by_dict(articles_2017, dates_2017)\n",
    "\n",
    "    #向量化Training Data\n",
    "    all_tfidf_list = all_tfidf_list_forModel(dates_2017, articles_dict, feature_words_selection)\n",
    "    return all_tfidf_list, articles_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 3、構建分類模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stockPrediction(all_tfidf_list, articles_label):    \n",
    "    #MutilnomiaNB分类方式\n",
    "    clf = MultinomialNB(alpha=0.001).fit(all_tfidf_list[:200], articles_label[:200])\n",
    "    #传入的数据是，列为feature的个数，行为某个要分类的文本\n",
    "    nb_prediction = clf.predict(all_tfidf_list[200:])\n",
    "    print(nb_prediction)\n",
    "    nbRate = trueRate(nb_prediction, articles_label[200:])\n",
    "    print(nbRate)\n",
    "\n",
    "    #svm分类方式，Support Vector Machine\n",
    "    svm_clf = svm.SVC().fit(all_tfidf_list[:200], articles_label[:200])\n",
    "    svm_prediction = svm_clf.predict(all_tfidf_list[200:])\n",
    "    print(svm_prediction)\n",
    "    svmRate = trueRate(svm_prediction, articles_label[200:])\n",
    "    print(svmRate)\n",
    "\n",
    "    #高斯分类\n",
    "    gnb_clf = GaussianNB().fit(all_tfidf_list[:200], articles_label[:200])\n",
    "    gnb_prediction = gnb_clf.predict(all_tfidf_list[200:])\n",
    "    print(gnb_prediction)\n",
    "    gnbRate = trueRate(gnb_prediction, articles_label[200:])\n",
    "    print(gnbRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_aricles = '/Users/vincentyau/Documents/台大硕一下资料/大数据与商业分析/final_version/social_d.xlsx'\n",
    "path_vocabulary = '/Users/vincentyau/Documents/台大硕一下资料/大数据与商业分析/final_version/2912_price_up_term.xlsx'\n",
    "path_label = '/Users/vincentyau/Documents/台大硕一下资料/大数据与商业分析/final_version/2912_s_17_label.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_tfidf_list, articles_label = get_all_tfidf_list(path_aricles, path_vocabulary, path_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U'\n",
      " 'U' 'U' 'U' 'U' 'U' 'U']\n",
      "0.625\n",
      "['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U'\n",
      " 'U' 'U' 'U' 'U' 'U' 'U']\n",
      "0.625\n",
      "['D' 'D' 'D' 'D' 'D' 'D' 'U' 'D' 'D' 'D' 'D' 'U' 'D' 'D' 'U' 'U' 'U' 'D'\n",
      " 'U' 'D' 'D' 'D' 'D' 'D']\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "stock_2017 = stockPrediction(all_tfidf_list, articles_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
